package server

import (
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/Heng-Bian/memory-chat/pkg/llm"
	"github.com/Heng-Bian/memory-chat/pkg/memory"
	"github.com/Heng-Bian/memory-chat/pkg/types"
)

// Server OpenAIå…¼å®¹çš„HTTPæœåŠ¡å™¨
type Server struct {
	llmClient      llm.Client
	memoryManagers map[string]*memory.Manager
	memoryDir      string
}

// NewServer åˆ›å»ºæ–°çš„æœåŠ¡å™¨
func NewServer(llmClient llm.Client, memoryDir string) *Server {
	return &Server{
		llmClient:      llmClient,
		memoryManagers: make(map[string]*memory.Manager),
		memoryDir:      memoryDir,
	}
}

// getMemoryManager è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„è®°å¿†ç®¡ç†å™¨
func (s *Server) getMemoryManager(userID string) *memory.Manager {
	// éªŒè¯ userID é˜²æ­¢è·¯å¾„éå†æ”»å‡»
	if strings.Contains(userID, "..") || strings.Contains(userID, "/") || strings.Contains(userID, "\\") {
		userID = "invalid_user"
	}
	
	if mm, exists := s.memoryManagers[userID]; exists {
		return mm
	}

	storePath := fmt.Sprintf("%s/%s.yaml", s.memoryDir, userID)
	mm := memory.NewManager(userID, s.llmClient, storePath)
	if err := mm.Load(); err != nil {
		// è®°å½•åŠ è½½é”™è¯¯ä½†ç»§ç»­ä½¿ç”¨ç©ºè®°å¿†
		fmt.Printf("Warning: failed to load memory for user %s: %v\n", userID, err)
	}

	s.memoryManagers[userID] = mm
	return mm
}

// ChatCompletionRequest OpenAIèŠå¤©è¯·æ±‚æ ¼å¼
type ChatCompletionRequest struct {
	Model    string          `json:"model"`
	Messages []types.Message `json:"messages"`
	Stream   bool            `json:"stream,omitempty"`
	UserID   string          `json:"user,omitempty"` // ç”¨äºè®°å¿†ç®¡ç†
}

// ChatCompletionResponse OpenAIèŠå¤©å“åº”æ ¼å¼
type ChatCompletionResponse struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int64  `json:"created"`
	Model   string `json:"model"`
	Choices []struct {
		Index   int           `json:"index"`
		Message types.Message `json:"message"`
		FinishReason string   `json:"finish_reason"`
	} `json:"choices"`
	Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	} `json:"usage"`
}

// ChatCompletionStreamResponse OpenAIæµå¼å“åº”æ ¼å¼
type ChatCompletionStreamResponse struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int64  `json:"created"`
	Model   string `json:"model"`
	Choices []struct {
		Index int `json:"index"`
		Delta struct {
			Content string `json:"content,omitempty"`
			Role    string `json:"role,omitempty"`
		} `json:"delta"`
		FinishReason string `json:"finish_reason,omitempty"`
	} `json:"choices"`
}

// HandleChatCompletions å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚
func (s *Server) HandleChatCompletions(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req ChatCompletionRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, fmt.Sprintf("Invalid request: %v", err), http.StatusBadRequest)
		return
	}

	// å¦‚æœè¯·æ±‚åŒ…å«ç”¨æˆ·IDï¼Œä½¿ç”¨è®°å¿†ç®¡ç†å™¨
	var contextMessages []types.Message
	var mm *memory.Manager

	if req.UserID != "" {
		mm = s.getMemoryManager(req.UserID)

		// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
		if len(req.Messages) > 0 {
			lastMsg := req.Messages[len(req.Messages)-1]
			if lastMsg.Role == "user" {
				mm.AddMessage(lastMsg.Role, lastMsg.Content)
			}
		}

		// è·å–åŒ…å«å†å²è®°å¿†çš„ä¸Šä¸‹æ–‡
		contextMessages = mm.GetContextMessages()
	} else {
		contextMessages = req.Messages
	}

	// æµå¼å“åº”
	if req.Stream {
		s.handleStreamResponse(w, r, req, contextMessages, mm)
		return
	}

	// éæµå¼å“åº”
	s.handleNormalResponse(w, req, contextMessages, mm)
}

// handleNormalResponse å¤„ç†éæµå¼å“åº”
func (s *Server) handleNormalResponse(w http.ResponseWriter, req ChatCompletionRequest, contextMessages []types.Message, mm *memory.Manager) {
	response, tokens, err := s.llmClient.Chat(contextMessages)
	if err != nil {
		http.Error(w, fmt.Sprintf("LLM error: %v", err), http.StatusInternalServerError)
		return
	}

	// å¦‚æœä½¿ç”¨è®°å¿†ç®¡ç†å™¨ï¼Œä¿å­˜åŠ©æ‰‹å“åº”
	if mm != nil {
		mm.AddMessage("assistant", response.Content)
		if err := mm.Save(); err != nil {
			fmt.Printf("Warning: failed to save memory: %v\n", err)
		}
	}

	// æ„é€ OpenAIæ ¼å¼çš„å“åº”
	resp := ChatCompletionResponse{
		ID:      fmt.Sprintf("chatcmpl-%d", time.Now().Unix()),
		Object:  "chat.completion",
		Created: time.Now().Unix(),
		Model:   req.Model,
		Choices: make([]struct {
			Index        int           `json:"index"`
			Message      types.Message `json:"message"`
			FinishReason string        `json:"finish_reason"`
		}, 1),
	}

	resp.Choices[0].Index = 0
	resp.Choices[0].Message = *response
	resp.Choices[0].FinishReason = "stop"
	resp.Usage.TotalTokens = tokens
	resp.Usage.PromptTokens = tokens / 2
	resp.Usage.CompletionTokens = tokens / 2

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(resp)
}

// handleStreamResponse å¤„ç†æµå¼å“åº”ï¼ˆSSEï¼‰
func (s *Server) handleStreamResponse(w http.ResponseWriter, r *http.Request, req ChatCompletionRequest, contextMessages []types.Message, mm *memory.Manager) {
	// è®¾ç½®SSE headers
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("Access-Control-Allow-Origin", "*")

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, "Streaming not supported", http.StatusInternalServerError)
		return
	}

	requestID := fmt.Sprintf("chatcmpl-%d", time.Now().Unix())
	created := time.Now().Unix()

	// å‘é€åˆå§‹è§’è‰²
	initialResp := ChatCompletionStreamResponse{
		ID:      requestID,
		Object:  "chat.completion.chunk",
		Created: created,
		Model:   req.Model,
		Choices: make([]struct {
			Index int `json:"index"`
			Delta struct {
				Content string `json:"content,omitempty"`
				Role    string `json:"role,omitempty"`
			} `json:"delta"`
			FinishReason string `json:"finish_reason,omitempty"`
		}, 1),
	}
	initialResp.Choices[0].Delta.Role = "assistant"
	sendSSE(w, flusher, initialResp)

	// ç´¯ç§¯å®Œæ•´å“åº”ç”¨äºä¿å­˜åˆ°è®°å¿†
	var fullContent strings.Builder

	// æµå¼å‘é€å“åº”
	tokens, err := s.llmClient.ChatStream(contextMessages, func(content string) error {
		fullContent.WriteString(content)

		streamResp := ChatCompletionStreamResponse{
			ID:      requestID,
			Object:  "chat.completion.chunk",
			Created: created,
			Model:   req.Model,
			Choices: make([]struct {
				Index int `json:"index"`
				Delta struct {
					Content string `json:"content,omitempty"`
					Role    string `json:"role,omitempty"`
				} `json:"delta"`
				FinishReason string `json:"finish_reason,omitempty"`
			}, 1),
		}
		streamResp.Choices[0].Delta.Content = content

		return sendSSE(w, flusher, streamResp)
	})

	if err != nil {
		// å‘é€é”™è¯¯å¹¶å…³é—­æµ
		fmt.Fprintf(w, "data: {\"error\": \"%v\"}\n\n", err)
		flusher.Flush()
		return
	}

	// å¦‚æœä½¿ç”¨è®°å¿†ç®¡ç†å™¨ï¼Œä¿å­˜åŠ©æ‰‹å“åº”
	if mm != nil {
		mm.AddMessage("assistant", fullContent.String())
		if err := mm.Save(); err != nil {
			fmt.Printf("Warning: failed to save memory: %v\n", err)
		}
	}

	// å‘é€ç»“æŸä¿¡å·
	finalResp := ChatCompletionStreamResponse{
		ID:      requestID,
		Object:  "chat.completion.chunk",
		Created: created,
		Model:   req.Model,
		Choices: make([]struct {
			Index int `json:"index"`
			Delta struct {
				Content string `json:"content,omitempty"`
				Role    string `json:"role,omitempty"`
			} `json:"delta"`
			FinishReason string `json:"finish_reason,omitempty"`
		}, 1),
	}
	finalResp.Choices[0].FinishReason = "stop"
	sendSSE(w, flusher, finalResp)

	// å‘é€[DONE]
	fmt.Fprintf(w, "data: [DONE]\n\n")
	flusher.Flush()

	_ = tokens // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¯ä»¥è®°å½•tokenä½¿ç”¨æƒ…å†µ
}

// sendSSE å‘é€SSEäº‹ä»¶
func sendSSE(w io.Writer, flusher http.Flusher, data interface{}) error {
	jsonData, err := json.Marshal(data)
	if err != nil {
		return err
	}

	fmt.Fprintf(w, "data: %s\n\n", jsonData)
	flusher.Flush()
	return nil
}

// HandleHealth å¥åº·æ£€æŸ¥ç«¯ç‚¹
func (s *Server) HandleHealth(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{
		"status": "ok",
		"time":   time.Now().Format(time.RFC3339),
	})
}

// Start å¯åŠ¨HTTPæœåŠ¡å™¨
func (s *Server) Start(addr string) error {
	http.HandleFunc("/v1/chat/completions", s.HandleChatCompletions)
	http.HandleFunc("/health", s.HandleHealth)

	fmt.Printf("ğŸš€ HTTPæœåŠ¡å™¨å¯åŠ¨åœ¨ %s\n", addr)
	fmt.Println("ç«¯ç‚¹:")
	fmt.Println("  - POST /v1/chat/completions (OpenAIå…¼å®¹)")
	fmt.Println("  - GET  /health (å¥åº·æ£€æŸ¥)")
	fmt.Println()

	return http.ListenAndServe(addr, nil)
}
